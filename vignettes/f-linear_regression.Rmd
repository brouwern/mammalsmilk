---
title: "f) Linear Regression "
author: "brouwern@gmail.com"
date: "January 30, 2017"
output:
  pdf_document: default
  word_document:
    fig_caption: yes
    fig_height: 5
    fig_width: 9
    reference_docx: rmarkdown_template.docx
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, strip.white = FALSE, cache = TRUE)
```




## What does regression do?

Regression typically is involved in 3 potentially related tasks

1. Inference
1. Prediction
1. Model selection

For the milk data, relevant aspects of these tasks might be:

* INFERENCE: is the evolution of milk fat drive by change in body size?
* PREDICTION: i.e., given the body size of an extinct animal, what do we ...
    + predict its % milk fat to be?
    + how certain could we be in this prediction?
* MODEL SELECTION: what's the best predictor of milk fat?
    + body size?
    + litter size?
    + lactation duration?


**Prediction** can be agnostic to inference; the prediction just has to be accurate, regardless of the actual causal relationship between the y and the x variable.  Stated another way, prediction is about the statistical relationship between 2 variables.

**Model selection** can be used for inference or prediction.  For inference, the "best" model is considered to have, *relative to the other models*, a higher chance of representing causal relationships.  However, if a model the corresponds to reality isn't included in the set of models examined, any inference about causation will be spurious.  Moreover, investigation about causation is always best grounded in experimentation, and the milk dataset.   For prediction, model selection is about finding the strongest statistical association between y and x variables

## Steps in regression

* Standard approach
    + Model fitting: least squares
    + Inference: Frequentist
    + (In particular: "NHST" w/alpha = 0.05)
    + (="Null hypothesis significance testing")
* More advanced models: 
    + Model fitting: "GLS"
    + "generalized least square"
    + gls() function, nlme package
    + can relax standard regression assumptions, such as constancy of variance or independence of points
    + This tool isn't used by bio/ecologists much, but should be!
    + The exception to this is comparative studies like Skibiel which should take into account phylogeny
* Advanced models: Likelihood
    + For "GLMs": generalized linear models
    + i.e. logistic regression
    + repeated measures, mixed effects, random effects often fit with likelihood methods
* Very Complex models: Bayes w/ MCMC
    + very complex/hierarchical or multilevel models
    + MCMC: Markov chain Monte carol




## Aside: what do I mean by "multilevel" or "hiearchical"

**What is it?**

* Many names for same / similar issue
    + repeated measures
    + blocking
    + random effects / mixed effects models
    + multilevel models
    + hierarchical models
* Measurements on same thing multiple times or on similar experimental units are likely to be more similar to each other than random
* Violates the assumption of independence in regression/ANOVA



### Multilevel Examples

* Education studies
    + students in class rooms in schools in districts in states
    + (multiple students per class, multiples classes per school, multiple schools per district)
* Health care studies
    + patients on hospital floors in hospitals in cities
* Animal studies
    + repeated measures on mice in cages
    + (multiple measurements per mice, multiple mice per cage)




## Regression models in R using lm()


### Focal data subset: Primates & Relatives

* Primates
* Rodents (this is why we use mouse models)
* Rabbits

```{r}
file. <- "skibiel_primate_fat.csv"

full.path <- here::here("data", #folder
                        file.)  #file name

primate.milk <- read.csv(file = full.path)
```


### Basic R regression


* "lm()" = "linear model""
* regression, t.test, ANOVA, ANCOVA are all linear models
* all are fit with lm() function
* (Welch's correct only done with t.test; similar effect with gls())
* "gls()" = generalized least squares
* "glm()" = generalized linear model


```{r}
lm.mass <- lm(fat ~ log(mass.fem),
             data = primate.milk)
```




## Model summary

* Look at model w/ summary()
* (we can get just the coefficients/slopes with coef() )
* (another useful function is arm::display; has shorter output)
* (recall that you can save these summary outputs and access them as lists with a $)

```{r, echo = -1}
options(digits=3)
summary(lm.mass)
```

We'll unpack what each of these is



### Plot model w/ sumamry output
 
```{r, echo = F}

mar.default <- c(c(5, 4, 4, 1) + 0.1)
par(mfrow = c(1,1), mar = c(5, 4, 4, 2) + 0.1)

plot(fat ~ log(mass.fem),
     data = primate.milk,
     ylab = "Fat content of milk",
     pch = 18,
     cex =2)
abline(lm.mass, col =3, lwd  =2)

##m.coefs(lm.mass)

```





### Plot model w/ equations

```{r, echo = F}
par(mfrow = c(1,1))
# source("./scripts_Lecture4/sx_mtext_plot_equations_in_margin.R")
plot(fat ~ log(mass.fem),
     data = primate.milk,
     ylab = "Fat content of milk",
     pch = 18,
     cex =2)
abline(lm.mass, col =3, lwd  =2)
#m.equations(lm.mass)

```





```{r, echo = F}
### Plot model w/ wider x limits
#function to make plot
make.plot <- function(){
  plot(fat ~ log(mass.fem),
       data = primate.milk,
       ylab = "Fat content of milk",
       pch = 18,
       cex =2,
       xlim = c(0,12))
  abline(lm.mass, col =3, lwd  =2)
}

```




### Add location of intercept

```{r, echo=F}
par(mfrow = c(1,1), mar = c(4.1,4.1,4,1))
make.plot()
y. <- coef(lm.mass)[1]
abline(v = 0, col = 2,lwd = 2)
arrows(x0 = 1,x1=0,
       col = 2,lwd = 2,
       y0 = y.)
lab. <- round(coef(lm.mass)[1],2)
text(x = 1.71,y = y.,
     label = lab.)
#m.coefs(lm.mass)
```




# Coefficient plot
## What is this?

* arm::coefplot()
* NOTE: by convention, axes are flipped!

```{r, echo = F}
library(coefplot)
arm::coefplot(lm.mass,intercept = T,
              mar=c(1,9,5.1,2),
              cex.pts = 2,
              col.pts = 2:1,
              pch.pts = 16:17,
              lwd = 3)
mtext(text = "y  = m*x + b",
      side = 1,adj= 1, line = -2)
mtext(text = "fat = slope*log(mass) + intercept",side = 1,adj= 1, line = -1)
#mtext(text = mod,side = 1,adj= 1, line = 0)


```




# Coefficient plot
## What is this?

* arm::coefplot()
* NOTE: by convention, axes are flipped!

```{r, echo = F}
library(coefplot)
arm::coefplot(lm.mass,intercept = T,
              mar=c(1,9,5.1,2),
              cex.pts = 2,
              col.pts = 2:1,
              pch.pts = 16:17,
              lwd = 3)
mtext(text = "y  = m*x + b",
      side = 1,adj= 1, line = -2)
mtext(text = "fat = slope*log(mass) + intercept",side = 1,adj= 1, line = -1)
#mtext(text = mod,side = 1,adj= 1, line = 0)

text(x  = 1, y = 2, label = "Slope of line",pos = 4, cex = 2)

```




## Compare coefficient plot to raw data

```{r}
par(mfrow = c(1,2), mar = c(5,2,7,0.25))

arm::coefplot(lm.mass,intercept = T,
              mar=c(5,6,8,1),
              cex.pts = 2,
              col.pts = 1:2,
              pch.pts = 16:17,
              lwd = 3)
mtext(text = "y  = m*x + b",
      side = 1,adj= 1, line = -2)
mtext(text = "fat = slope*log(mass) + intercept",side = 1,adj= 1, line = -1)
make.plot()

y. <- coef(lm.mass)[1]
abline(v = 0, col = 2,lwd = 2)
arrows(x0 = 1,x1=0,
       col = 2,lwd = 2,
       y0 = y.)
lab. <- round(coef(lm.mass)[1],2)
text(x = 1.71,y = y.,
     label = lab.)

```





# Linear regression as model fitting

## Fit 2 models

### Null model (Ho): flat line

* Null hypothesis: ????
* Null model: "fat ~ 1"
* "~1" means fit model 
    + w/ slope = 0 
    + intercept = mean of response variable

```{r}
lm.null <- lm(fat ~ 1,
              data = primate.milk)

```


### Alternative model: fat ~ log(mass.fem)

```{r}

lm.mass <- lm(fat ~ log(mass.fem),
              data = primate.milk)


```







## Plot both models

```{r, echo = F}
par(mfrow = c(1,2),mar = c(4.1,4.1,2,1))
plot(fat ~ log(mass.fem),
     data = primate.milk,
     ylab = "Fat content of milk",
     pch = 18,
     cex =2)
mtext("Ho: Null model",cex = 1.5)
abline(lm.null, col =4, lwd  =3, lty = 3)
legend("topright",
       col = c(4),
       lty = 3,
       lwd = 4,
       legend = c("y ~ 0*log(mass) + int.null"))

plot(fat ~ log(mass.fem),
     data = primate.milk,
     ylab = "",
     pch = 18,
     cex =2)
mtext("Ha: Alt model",cex = 1.5)
abline(lm.mass, col =3, lwd  =3)

legend("topright",
       col = c(3,4),
       lty = 1:2,
       lwd = 3,
       legend = c("y ~ slope*log(mass) + int.alt"))
```






## What is the intercept of null model?

The mean fat value is 8.6
```{r}
summary(primate.milk[,"fat"])
```


```{r, echo = F}
par(mfrow = c(1,2))
plot(fat ~ log(mass.fem),
     data = primate.milk,
     ylab = "Fat content of milk",
     pch = 18,
     cex =2,
     main = "")
abline(lm.null, col =3, lwd  =3)
mtext("Ho: Null model",cex = 1.5)
legend("topright",
       legend = c("y ~ 0 * log(mass) + 8.6"))


plot(fat ~ log(mass.fem),
     data = primate.milk,
     ylab = "Fat content of milk",
     pch = 18,
     cex =2,
     main = "")
abline(lm.mass, col =4, lwd  =3)
mtext("Ha: Alt model",cex = 1.5)
legend("topright",
       legend = c("y ~ -1.75 * log(mass) + 20.5"))
```








# Test model: significance test

## Nested Models
### Ha: y ~ -1.75*log(mass) + 20.5
### Ho: y ~     0*log(mass) +  8.6


* Ha has TWO parameters (aka coefficients, betas)
    + Intercept = `r round(coef(lm.mass)[1],3)`
    + Slope     = -1.75
* Ho has ONE parameter
    + Intercept = 8.6
    + (Slope = 0, so it doesn't count)
* Hypothesis can be formulated of 2 "nested models"
    + Applies to t-test, ANOVA, ANCOVA
    + A major difference between Hypothesis testing w/p-values and IT-AIC is that AIC doesn't require models to be nested!




## Hypothesis test of nested models in R

* anova() 
* carries out an F test for linear models
* "likelihood ratio test" for GLMs/GLMMs
    + GLMM = generalized linear mixed models
    + Models w/random effects, blocking, repeated measures etc
    
```{r}
#NOTE: order w/in anova() doesn't matter
anova(lm.null, 
      lm.mass)
```

* Output of anova() is similar to content of summary()
* I like to emphasize the testing of nested models
* GLMs, GLMMs often can only be tested with anova()





# Examine model: IT-AIC

## IT-AIC

* "Information Theory- Akaike's Information Criteria"
* "Information Theory" is a general approach to investigating models
* AIC is a particular information theory
    + Others: BIC, WAIC, FIC, DIC (Bayesian), ...
    + seems like every statistician has one now
* AICc: correction for small samples size
    + Should always use AICc
* qAIC: correction for "overdispersion"
    + For GLMs (i.e.. logistic regression)

## AIC in base R

* Note: no AICc command in base R!
* Other packages can do AICc and other ICs
* DF = number of parameters
```{r}
AIC(lm.null, 
    lm.mass)
```





## AIC table

* Lower is "better"
* Absolute values have NO MEANSING
* AICs are only interpretable relative to other AIC
* focus: "delta AIC" (dAIC)
* AIC goes down if a model fits the data better
* BUT - every additional parameter has a "penalty"

```{r}
library(bbmle)

AICtab(lm.null,
       lm.mass,
       base = T)
```



## AICc 

* AICc is a correction for small sample size
* Doesn't make much difference here

```{r}
library(bbmle)

ICtab(lm.null,
      lm.mass,
      type = c("AICc"),
      base = T)
```




# What about R^2?

### Hypothesis testing:
* p-values: tell if you if a predictor is "significantly" associated w/ a response
* Does not tell you the EFFECT SIZE is large
    + p can be very very small
    + but slope also very small
    
### IC-AIC
* AIC tells you if one model fits the data better than another
* AIC doesn't tell you if either model data very well

### R^2
* Tells you how much variation in the data is explained by model
* whether you use p-values or AIC, you should report R^2
    






## Null vs Alternative model

* Slope of Alt models is highly significant
* But R2 isn't huge
* Lots of scatter around line
* Other predictors might improve fit
```{r, echo = F}

par(mfrow = c(1,2),
    mar = c(4,4,2,1))
R2.null <- paste("R2 =",summary(lm.null)$r.squared)

plot(fat ~ log(mass.fem),
     data = primate.milk,
     ylab = "Fat content of milk",
     pch = 18,
     cex =2,
     main = "Null model (Ho)")
abline(lm.null, col =3, lwd  =3)
legend("topright",
       legend = R2.null)

R2.Ha <- paste("R2 =",
               round(summary(lm.mass)$r.squared,2)
               )
plot(fat ~ log(mass.fem),
     data = primate.milk,
     ylab = "Fat content of milk",
     pch = 18,
     cex =2,
     main = "Alt. model (Ha)")
abline(lm.mass, col =4, lwd  =3)
legend("topright",
       legend = R2.Ha)
```
